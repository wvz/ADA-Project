In the last part of our project, we asked the question if we can create an intellectual tag recommendation engine for StackOverflow. Because when people ask questions on StackOverflow, they've got to choose tags by themselves, and sometimes they don’t really know what tags they should choose. So here we develop a tag recommendation engine to help them out by implementing LDA topic modeling and calculating weighted probability of tags of the closest training documents within the given topic. 

First, I will introduce a little bit about LDA topic modeling, which is a Bayesian clustering technique widely used for inferring hidden ‘topics’ in document corpora. The basic idea is that each document can be represented as a mixture of K topics, and every topic has a discrete distribution over words. By fitting LDA model on the body text of R questions, and here we tuned K = 50, we got document distribution in 50 topics and word distribution in each topic. We can get a sense of what these 50 topic look like by looking at this graph. On the left hand side, each circle represents a certain topic. The size of the circles represents the number of documents in that topic. if you click one of the circles i.e. topics, on the right hand side, you will see the top 30 most prominent terms in that topic, which gives you a sense of what that topic is talking about. For example, (hover over topic 3) if we click topic 3, we’ll see this topic is mainly about plotting, since we have ‘ggplot’ and all the options here. 

After fitting the LDA model, we can predict which topic a new document belongs to. Let’s take a cleaned question in test set for example. It belongs to Topic 15. After we know its topic, we can shrink our search range from all 140K+ documents to averagely 3K documents of the same topic, which would make our recommendation engine less time-costly and memory-consuming for our client. In our example, there are 5K documents in that topic, which is still too many. Therefore, we want to find 20 questions that are most similar to the untagged question. The similarity of two questions we chose here is Jaccard Index, which is the ratio of the length of their intersection set to the length of their union set. After we pick out the 20 closest documents to the untagged question, we count their tag frequencies and sort tags by descending order. By that, we get a list of tags we have confidence to recommend. But we didn’t stop here. We thought it would make more sense if some words in the question body also appeared in recommended tags. So we decided to give ‘bonus’ to the tags that appeared in the question body. In this example, the tags marked blue here appeared in the question body, so we increase their frequencies by 10 which is tuned, and then sort the tags again. We get the next table, and we recommend the top 6 tags to this question, finally.

We have repeated this process for the whole test set, and the accuracy of recommending correct tags is 71%, which we think is pretty good.

